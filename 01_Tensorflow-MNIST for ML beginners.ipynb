{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST For ML Beginners \n",
    "-------------------------------\n",
    "[Tutorial link](https://www.tensorflow.org/get_started/mnist/beginners)\n",
    "\n",
    "\n",
    "\n",
    "### Dependencies\n",
    " - python 3.5, 3.6\n",
    " - tensorflow 1.2\n",
    " - tqdm (pip install tqdm): visualization tools for a loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create simple linear classifier\n",
    "-----------------------------------\n",
    "28 x 28 MNIST Image, first dimension is **'None'**, because we feed a batch of images (mini-batch) to our classification layer. Tensorflow will change dimension to multifly matirx onto this input batch. It's called **'broadcasting.'**\n",
    "\n",
    "https://www.tensorflow.org/performance/xla/broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([784, 10], -0.1, 0.1))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # input images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax function\n",
    "----------------------------------\n",
    "https://en.wikipedia.org/wiki/Softmax_function  \n",
    "http://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\boldsymbol{y} & = softmax(W * \\boldsymbol{x} + \\boldsymbol{b}) \\\\\n",
    "y_i & = \\frac {e^{z_i}} {\\sum_{k=1}^K e^{z_k}}  \\qquad\\textrm{where}\\  \\boldsymbol{z} = W * \\boldsymbol{x} + \\boldsymbol{b} \\\\\n",
    "\\end{split}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define cross entropy\n",
    "------------------------------\n",
    "\n",
    "[The entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory) is defined as follows:\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "H(\\boldsymbol{p}) = -\\sum \\boldsymbol{p}\\log{\\boldsymbol{p}} \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[The cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) is defined as follows:\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "H(\\boldsymbol{p},\\boldsymbol{q}) & = H(P) + D_{KL}(\\boldsymbol{p}||\\boldsymbol{q}) \\\\\n",
    "& = -\\sum \\boldsymbol{p}\\log{\\boldsymbol{p}} + \\sum \\boldsymbol{p} \\log{\\frac{\\boldsymbol{p}}{\\boldsymbol{q}}} \\\\\n",
    "& = -\\sum \\boldsymbol{p} \\log(\\boldsymbol{q})\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss\n",
    "-----------------------------\n",
    "We define our loss function using the cross entropy as follows:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "L = -\\frac{1}{N}\\sum \\boldsymbol{y'}\\log{\\boldsymbol{y}}\n",
    "  \\qquad \\textrm{where} \\ N \\ \\textrm{is size of mini-batch}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10]) # corrent labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross-entropy\n",
    "# 'axis=1' indicates that summation over each example\n",
    "H = - tf.reduce_sum(y_ * tf.log(y), axis=1) \n",
    "\n",
    "# the mean over all the examples in the batch\n",
    "# Note that these equations have numerical unstability on x < 0.\n",
    "# So, tensorflow provides helper function to deal with it, tf.nn.softmax_cross_entropy_with_logits\n",
    "# Next tutorial, you can see how to use this function.\n",
    "L = tf.reduce_mean(H) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note the tensor shapes\n",
    "print(y_)\n",
    "print(H)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create gradient descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 'InteractiveSession' for interactive python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run() # init all variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check our variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('W: \\n', W.eval()) # randomly initialized\n",
    "print('b: \\n', b.eval()) # zero constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for iteration in tqdm(range(1000)):\n",
    "    images, labels = mnist.train.next_batch(128) # get mini-batch images and corresponding labels\n",
    "    _, loss = sess.run([optimizer, L], feed_dict={x: images, y_: labels})\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        print (\"iter : {:4d},  loss: {:.5f}\".format(iteration, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# after training\n",
    "print('W: \\n', W.eval()) \n",
    "print('b: \\n', b.eval()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36-tensorflow-gpu",
   "language": "python",
   "name": "py36-tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

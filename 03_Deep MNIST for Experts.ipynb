{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST convolutional neural network   \n",
    "\n",
    "----------------------\n",
    "[Tutorial link](https://www.tensorflow.org/get_started/mnist/pros)\n",
    "\n",
    "Dependencies\n",
    "- Python 3.5, 3.6\n",
    "- Tensorflow 1.2\n",
    "- tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Multilayer Convolutional Network\n",
    "--------------------------------------\n",
    "1. reshape MNIST input data\n",
    "2. layer1: 5x5 convolution and pooling\n",
    "3. layer2: 3x3 convolution and pooling\n",
    "4. flatten (reshape) activation map\n",
    "5. layer3: 1024 fully connected layer\n",
    "6. output layer: 10 fully connected classification layer\n",
    "\n",
    "We will use 'tf.reshape', 'tf.nn.conv2d', 'tf.relu', 'tf.dropout', and 'tf.nn.softmax_cross_entropy_with_logits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0. set placeholder for input data and dropout probability\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_label = tf.placeholder(tf.float32, [None, 10])\n",
    "dropout_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. reshape MNIST input data\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1]) # 'NHWC' format (batch_size, height, width, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. layer1: 5x5 convolution and pooling\n",
    "W_conv1 = tf.Variable(tf.random_normal([5, 5, 1, 32], stddev=0.2), # convolution with 5x5 filter, 32 features\n",
    "                      name='conv1_weight', dtype=tf.float32)\n",
    "b_conv1 = tf.Variable(tf.zeros([32]), name='conv1_bias')\n",
    "conv1 = tf.nn.conv2d(x_image, W_conv1, [1,1,1,1], padding='SAME')\n",
    "conv1 = tf.nn.bias_add(conv1, b_conv1)\n",
    "conv1 = tf.nn.relu(conv1) # add activation function\n",
    "pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "pool1 = tf.nn.dropout(pool1, dropout_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. layer2: 3x3 convolution and pooling\n",
    "W_conv2 = tf.Variable(tf.random_normal([3, 3, pool1.get_shape().as_list()[-1], 64], stddev=0.2),\n",
    "                      name='conv2_weight', dtype=tf.float32)\n",
    "b_conv2 = tf.Variable(tf.zeros([64]), name='conv2_bias')\n",
    "conv2 = tf.nn.conv2d(pool1, W_conv2, [1,1,1,1], padding='SAME')\n",
    "conv2 = tf.nn.bias_add(conv2, b_conv2)\n",
    "conv2 = tf.nn.relu(conv2)\n",
    "pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "pool2 = tf.nn.dropout(pool2, dropout_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. flatten (reshape) activation map\n",
    "total_dim = 1\n",
    "for dim in pool2.get_shape().as_list()[1:]:\n",
    "    total_dim *= dim\n",
    "print(total_dim)\n",
    "    \n",
    "pool2_flat = tf.reshape(pool2, [-1, total_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. layer3: 1024 fully connected layer\n",
    "W_fc1 = tf.Variable(tf.random_uniform([total_dim, 1024], -0.03, 0.03),\n",
    "                    name='fc1_weight', dtype=tf.float32)\n",
    "b_fc1 = tf.Variable(tf.zeros([1024]),  name='fc1_bias')\n",
    "fc1 = tf.matmul(pool2_flat, W_fc1) + b_fc1\n",
    "fc1 = tf.nn.relu(fc1)\n",
    "fc1 = tf.nn.dropout(fc1, dropout_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6. output layer: 10 fully connected classification layer\n",
    "W_fc2 = tf.Variable(tf.random_uniform([fc1.get_shape().as_list()[-1], 10], -0.08, 0.08),\n",
    "                   name='fc2_weight', dtype=tf.float32)\n",
    "b_fc2 = tf.Variable(tf.zeros([10]))\n",
    "fc2 = tf.matmul(fc1, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv1)\n",
    "print(pool1)\n",
    "print(conv2)\n",
    "print(pool2)\n",
    "print(fc1)\n",
    "print(fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logits needs unscaled log probability.\n",
    "# It means we need to feed output value itself. \n",
    "# So, we do not apply activation function to 'fc2'.\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_label, logits=fc2))\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prediction op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = tf.equal(tf.argmax(fc2,1), tf.argmax(y_label,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(20000)):\n",
    "    images, labels = mnist.train.next_batch(128)\n",
    "    if i % 1000 == 0:\n",
    "        pred, acc = sess.run([prediction, accuracy], feed_dict={x: images, y_label: labels, dropout_prob: 1.0})\n",
    "        print(\"step {: 5d}, train accuracy[{: 2d}/{: 3d}] {:.5f}\"\n",
    "              .format(i, pred.sum(), pred.shape[0], acc))\n",
    "    optimizer.run(feed_dict={x: images, y_label: labels, dropout_prob: 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, acc = sess.run([prediction, accuracy],\n",
    "                    feed_dict={x: mnist.test.images, \n",
    "                                y_label: mnist.test.labels,\n",
    "                                dropout_prob: 1.0})\n",
    "print(\"test accuracy[{: 5d}/{: 5d}] {:.5f}\"\n",
    "      .format(pred.sum(), pred.shape[0], acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save optimized model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "import os\n",
    "PATH = 'ckpt_mnist'\n",
    "if not os.path.exists(PATH):\n",
    "    os.mkdir(PATH)\n",
    "saver.save(sess, os.path.join(PATH, 'mnist_conv2d'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36-tensorflow-gpu",
   "language": "python",
   "name": "py36-tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
